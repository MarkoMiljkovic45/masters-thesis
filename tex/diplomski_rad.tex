\documentclass[diplomskirad, upload]{fer}
% Dodaj opciju upload za generiranje konačne verzije koja se učitava na FERWeb
% Add the option upload to generate the final version which is uploaded to FERWeb


% Ovdje dodati pakete npr. \usepackage{blindtext}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}

\DeclareMathOperator*{\argmin}{arg\,min}


%--- PODACI O RADU / THESIS INFORMATION ----------------------------------------

% Naslov na engleskom jeziku / Title in English
\title{Generative models for financial time series based on latent factors}

% Naslov na hrvatskom jeziku / Title in Croatian
\naslov{Generativni modeli za financijske vremenske nizove zasnovani na latentnim faktorima}

% Broj rada / Thesis number
\brojrada{1234}

% Autor / Author
\author{Marko Miljković}

% Mentor 
\mentor{doc.~dr.~sc.~Stjepan~Begušić}

% Datum rada na engleskom jeziku / Date in English
\date{February, 2026}

% Datum rada na hrvatskom jeziku / Date in Croatian
\datum{Veljača, 2026.}

%-------------------------------------------------------------------------------


\begin{document}


% Naslovnica se automatski generira / Titlepage is automatically generated
\maketitle


%--- ZADATAK / THESIS ASSIGNMENT -----------------------------------------------

% Zadatak se ubacuje iz vanjske datoteke / Thesis assignment is included from external file
% Upiši ime PDF datoteke preuzete s FERWeb-a / Enter the filename of the PDF downloaded from FERWeb
\zadatak{zadatak.pdf}


%--- ZAHVALE / ACKNOWLEDGMENT --------------------------------------------------

\begin{zahvale}
  % Ovdje upišite zahvale / Write in the acknowledgment
  TODO Zahvala
\end{zahvale}


% Odovud započinje numeriranje stranica / Page numbering starts from here
\mainmatter


% Sadržaj se automatski generira / Table of contents is automatically generated
\tableofcontents


%--- UVOD / INTRODUCTION -------------------------------------------------------
\chapter{Uvod}
\label{pog:uvod}

TODO Uvod.

%-------------------------------------------------------------------------------
\chapter{Metodologija}
\label{pog:metodologija}

Ovo poglavlje pokriva teoretsku osnovu potrebnu za razumijevanje implementiranog modela dubokog učenja te 
strukturu podataka na kojima se primjenjuje. Prvo ćemo definirati osnovne varijable koje ćemo se oslanjati
u ovom radu. Zatim ćemo proučiti njihova statistička svojstva. Nakon toga razmotrit ćemo faktorske modele
koji omogućuju modeliranje naših varijabli. Na kraju ćemo pokriti duboko učenje i specifičnu arhitekturu
koja će se koristiti u ovom radu.

\section{Povrati imovine}
\label{sec:povrati}

Vrijednosni papiri predstavljaju financijske instrumente koji potvrđuju određena imovinska ili
druga prava njihova vlasnika, poput prava na udio u vlasništvu poduzeća (dionice) ili prava na
povrat uloženih sredstava uz kamatu (obveznice). Njima se trguje na organiziranim tržištima kapitala,
poput burzi, gdje se kupnja i prodaja odvijaju putem ovlaštenih posrednika, a cijene se formiraju na
temelju ponude i potražnje. Tržišna cijena vrijednosnog papira u svakom trenutku odražava ravnotežu
između kupaca i prodavatelja, pri čemu se svaka realizirana transakcija bilježi kao nova referentna cijena.
Tržišni indeksi predstavljaju promjene vrijednosti grupe dionica ili drugih financijskih
instrumenata. Služe kao mjerilo performansi tržišta ili određenog segmenta tržišta \cite{anafin02}.

Kako bi se omogućila analiza kretanja cijena kroz vrijeme, kontinuirani tok transakcijskih podataka
uzorkuje se u diskretnim vremenskim intervalima (npr. minute, sati, dani), čime se dobivaju vremenski
nizovi cijena koji služe kao temelj za statističku analizu i modeliranje. Slika \ref{fig:spy-price}
prikazuje kretanje dnevnih cijena za SPDR S\&P 500 ETF \engl{ exchange traded fund} od njegovog
začeća 1993. do kraja 2025. godine.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/spy_price_plot.pdf}
\caption{Kretanje cijene SPDR S\&P 500 ETF-a}
\label{fig:spy-price}
\end{figure}

U ovom radu koristit ćemo povrate umijesto samih cijena imovina. Postoje dva glavna razloga za to.
Prvi je to što povrati sažimaju kretanja cijena te ih stavljaju na jednaku i usporedivu skalu.
Drugi se odnosi na povoljnija statistička svojstva povrata u odnosu na cijene. Postoji više načina za
definirati povrate \cite{campbell1997}.


\subsubsection{Jednostavni (artimetički) povrati}
\label{subsubsec:artimeticki}

Nek je $P_t$ cijena imovine u trenutku $t$. Ako imovinu posjedujemo od trenutka $t-1$ do trenutka $t$,
ostvareni artimetički povrat dobivamo izrazom:
\begin{align}
  1 + R_t &= \frac{P_t}{P_{t-1}} \label{eq:artimeticki_gross} \\[12pt]
  R_t &= \frac{P_t}{P_{t-1}} - 1 = \frac{P_t - P_{t-1}}{P_{t-1}} \label{eq:artimeticki}
\end{align}

Ukoliko imovinu držimo kroz $T$ perioda ukupni artimetički povrat dobivamo ukamaćivanjem artimetičkih
povrata u vremenu \cite{tsay2010}:
\begin{equation} \label{eq:artimeticki-ukamacivanje}
\begin{split}
  1 + R_{total} = \frac{P_T}{P_1} &= \frac{P_T}{P_{T-1}} \times \frac{P_{T-1}}{P_{T-2}} \times ... \times \frac{P_2}{P_1} \\
  &= (1+R_T)(1+R_{T-1})\ ...\ (1+R_1) \\
  &= \prod_{t=1}^{T}(1 + R_t).
\end{split}
\end{equation}


\subsubsection{Kontinuirani (logaritamski) povrati}
\label{subsubsec:logaritamski}

Prirodni logaritam artimetičkog povrata (\ref{eq:artimeticki_gross}) je logaritamski povrat:
\begin{equation}
  r_t = \ln(1 + R_t) = \ln\frac{P_t}{P_{t-1}} = p_t - p_{t-1},
\label{eq:logartiamski}
\end{equation}
gdje $p_t = \ln(P_t)$.

Razmotrimo zatim ukamaćivanje logaritamskih povrata u vremenu:
\begin{equation} \label{eq:logaritamski-ukamacivanje}
\begin{split}
  r_{total} &= \ln(1 + R_{total}) = \ln[(1+R_T)(1+R_{T-1})\ ...\ (1+R_1)] \\
  &= \ln(1+R_T) + \ln(1+R_{T-1}) +\ ...\ + \ln(1+R_1) \\
  &= r_T + r_{T-1} + \ ... \ + r_1 \\
  &= \sum_{t=1}^{T}r_t
\end{split}
\end{equation}
Ukamaćivanje logaritamski povrata može se ostvariti zbrajanjem pojedinačnih logaritamskih
povrata. To svojstvo zovemo \emph{aditivnost u vremenu}. Logaritmski povrati također
posjeduju poželjna statistička svojstva \cite{tsay2010}. Za male magnitude vrijedi
$r_t \approx R_t$. Ova aproksimacija je korisna kada se razmatraju kratki vremenski
intervali \cite{anafin02}. Slika \ref{fig:spy-returns} prikazuje dnevne artimetičke
i logaritamske povrate SPDR S\&P 500 ETF. Sa slike vidimo da nema značajne
razlike između artimetičkih i logaritamskih povrata jer su povrati izračunati na kratkom
odnosno dnevnom vremenskom intervalu.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/spy_returns_plot.pdf}
\caption{Artimetički i logaritamski povrati SPDR S\&P 500 ETF-a}
\label{fig:spy-returns}
\end{figure}


\subsubsection{Povrat portfelja}
\label{subsubsec:portfelj}

Artimetički povrat portfelja koji se sastoji od $N$ vrijednosnica je otežana artimetička sredina
\engl{ weighted average} artimetičkih povrata vrijednosnica, gdje je težina \engl{ weight} pojedine
vrijednosnice njezin udio u vrijednosti portfelja \cite{tsay2010}.

Nek je $p$ portfelj kojem je $w_i$ ponder vrijednosnice $i$. Tada je artimetički povrat portfelja $p$ u
trenutku $t$ dan izrazom:
\begin{equation}
  R_{pt} = \sum_{i=1}^{N}w_iR_{it},
\label{eq:portfelj}
\end{equation}
gdje je $R_{it}$ artimetički povrat vrijednosnice $i$ u trenutku $t$. Ovo svojstvo nazivamo \emph{aditivnost
u prostoru vrijednosnica}. Logaritamski povrati ne posjeduju to svojstvo.


\subsubsection{Povrati iznad bezritične kamatne stope}
\label{subsubsec:excess}

Povrati iznad bezrizične kamatne stope \engl{ excess return} predstavlja razliku između ostvarenog
povrata određene imovine i referentnog, bezrizičnog povrata, te mjeri dodatnu kompenzaciju koju
investitor ostvaruje za preuzimanje rizika. Kao referentni povrat najčešće se koriste prinosi
na državne obveznice visoke kreditne kvalitete i kratkog dospijeća, jer se smatra da nose
zanemariv rizik \cite{tsay2010}.

\begin{equation}
  R_t^{excess} = R_t - R_f
\label{eq:excess}
\end{equation}


\section{Statistički modeli povrata}
\label{sec:statisticki}

Prije nego razmotrimo konkretne statističke modele povrata, potrebno je ukratko opisati osnovne
distribucije na kojima će se temeljiti daljnja analiza te načine estimacije njihovih parametara.

\subsubsection{Normalna distribucija}
\label{subsubsec:normal}

Normalna distribucija jedan je od najčešće korištenih probabilističkih modela u statistici i
financijama zbog svoje matematičke jednostavnosti i dobrih teorijskih svojstava.
Ako slučajna varijabla $X$ slijedi normalnu distribuciju sa sredinom $\mu$ i varijancom $\sigma^2$,
tada njezina funkcija gustoće ima oblik \cite{johnson1994vol1}:
\begin{equation}
  f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right).
\end{equation}

Za osmotreni uzorak $\{x_1, x_2, \dots, x_n\}$, procjena sredine dana je aritmetičkom sredinom
\begin{equation}
  \widehat{\mu} = \frac{1}{n}\sum_{i=1}^{n} x_i,
\label{eq:normalna-sredina}
\end{equation}
dok se varijanca procjenjuje izrazom:
\begin{equation}
  \widehat{\sigma^2} = \frac{1}{n}\sum_{i=1}^{n} (x_i - \widehat{\mu})^2.
\label{eq:normalna-varijanca}
\end{equation}


\subsubsection{Lognormalna distribucija}
\label{subsubsec:lognormal}

Lognormalna distribucija koristi se za modeliranje slučajnih varijabli čiji je logaritam
normalno distribuiran. Ako postoji broj $a$ takav da $Y=\ln(X - a)$ prati normalnu
distribuciju, slučajna varijabla $X$ tada slijedi lognormalnu distribuciju. Kako bi to vrijedilo,
vjerojatnost da slučajna varijabla $X$ poprimi vrijednost manju od $a$ mora biti jednaka nuli.
Ako vrijedi $Y \sim \mathcal{N}(\mu, \sigma^2)$, tada $X$ ima gustoću:
\begin{equation}
  f(x) = \frac{1}{(x-a)\sigma\sqrt{2\pi}} \exp\left[-\frac{(\ln(x - a) - \mu)^2}{2\sigma^2}\right]
  , \quad x > a.
\label{eq:lognormalna-gustoca}
\end{equation}

Sredina i varijanca lognormalne distribucije dane su izrazima
\begin{equation}
  E[X] = e^{\mu + \frac{\sigma^2}{2}}, \quad
  \mathrm{Var}[X] = (e^{\sigma^2}-1)e^{2\mu+\sigma^2}.
\end{equation}

Procjena parametara provodi se logaritamskom transformacijom uzorka, nakon čega se primjenjuju
standardne procjene za normalnu distribuciju \cite{johnson1994vol1}.


\subsubsection{Studentova t distribucija}
\label{subsubsec:studentt}

Studentova t distribucija predstavlja generalizaciju normalne distribucije s dodatnim parametrom
broja stupnjeva slobode $\nu$, koji kontrolira debljinu repova. Za manje vrijednosti $\nu$
distribucija ima izraženije repove, čime omogućuje robusnije modeliranje ekstremnih vrijednosti.

Gustoća Studentove t distribucije sa sredinom $\mu$, varijancom $\sigma^2$ i $\nu$ stupnjeva
slobode ima oblik:
\begin{equation}
f(x) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\sqrt{\nu\pi\sigma^2}}
\left(1 + \frac{(x-\mu)^2}{\nu\sigma^2}\right)^{-\frac{\nu+1}{2}}.
\end{equation}

Za procjenu parametara t distribucije koriste se metode procjene najveće izglednosti \engl{ maximum
likelihood estimation} \cite{johnson1994vol2}.


\subsubsection{Statistička svojstva povrata}

U statističkoj analizi financijskih vremenskih nizova često se polazi od pretpostavke da su aritmetički
povrati nezavisne i jednako distribuirane slučajne varijable s normalnom distribucijom,
konstantnom sredinom i varijancom, čime se značajno pojednostavljuje teorijska obrada i izvođenje
analitičkih rezultata. Međutim, takva pretpostavka suočava se s nizom ograničenja: aritmetički povrati
imaju donju granicu od $-1$, dok normalna distribucija nema ograničenja po realnoj osi,
višeperiodni povrati ne zadržavaju normalnu distribuciju zbog multiplikativne prirode
(\ref{eq:artimeticki-ukamacivanje}), a empirijski podaci često pokazuju odstupanja od normalnosti \cite{cont2001}.

Alternativno možemo pretpostaviti da su logaritamski povrati nezavisno i identično normalno
distribuirani sa sredinom $\mu$ i varijancom $\sigma^2$. S tom pretpostavkom impliciramo da
su aritmetički povrati nezavisno i identično lognormalno distribuirani.
Ovaj pristup ima povoljnija matematička svojstva jer su zbrojevi logaritamskih povrata kroz više
razdoblja također normalno distribuirani (\ref{eq:logaritamski-ukamacivanje}), a pritom se prirodno
zadovoljava donja granica aritmetičkog povrata (\ref{eq:lognormalna-gustoca}). Unatoč tim prednostima,
ni pretpostavka lognormalnosti u potpunosti ne opisuje realna tržišna kretanja, budući da stvarni
financijski povrati često pokazuju deblje repove distribucije i veću učestalost ekstremnih vrijednosti nego
što to predviđa normalna distribucija \cite{tsay2010}.

Kako bismo bolje obuhvatili ta empirijska svojstva, možemo koristiti studentovu $t$-distribuciju.
Slika \ref{fig:spy-returns-pdf} prikazuje distribuciju gustoće vjerojatnosti dnevnih
artimetičkih i logaritamskih povrata SPDR S\&P 500 ETF-a. Također su prikazane distribucije gustoće
vjerojatnosti normalne distribucije i studentove $t$-distribucije s pet stupnjeva slobode
\footnote{Biramo pet stupnjeva slobode kako bi imali konačna prva četiri momenta distribucije.
Za više informacija pogledati \cite{johnson1994vol2}}. Parametri $\mu$ i $\sigma^2$ za normalnu i
$t_5$-distribuciju procjenjeni su iz artimetičkih povrata za lijevi graf i logaritamskih povrata za
desni graf. Sa slike vidimo kako $t_5$-distribucija zbog svojih težih repova vijernije modelira
distribuciju povrata od normalne distribucije.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/spy_pdfs.pdf}
\caption{Distribucija dnevnih artimetičkih i logaritamskih povrata SPY ETF-a}
\label{fig:spy-returns-pdf}
\end{figure}


\subsubsection{Slučajni vektori}
\label{subsubsec:slucajni-vektori}

Nek je $\mathbf{X} = (X_1, \dots, X_p)$ slučajni vektor $p$ slučajnih varijabli.
Tada su vektor sredina i kovarijacijska matrica dani izrazima:
\begin{align}
  E(\mathbf{X}) &= \boldsymbol{\mu}_X = \left[E(X_1), \dots, E(X_p)\right]^\intercal \\
  \mathrm{Cov}(\mathbf{X}) &= \mathbf{\Sigma}_X =
  E\left[\left(\mathbf{X} - \boldsymbol{\mu}_X\right)
  \left(\mathbf{X} - \boldsymbol{\mu}_X\right)^\intercal\right]\text{,}
\end{align}
uz uvijet da dana očekivanj postoje. Neka su $\left\{\mathbf{x_1}, \dots, \mathbf{x_T}\right\}$
realizacije slučajnog vektora $\mathbf{X}$. Tada su uzoračka sredina i kovarijacijska matrica dane
izrazima \cite{tsay2010}:
\begin{equation}
  \widehat{\boldsymbol{\mu}}_x = \frac{1}{T}\sum_{t=1}^{T}\mathbf{x}_t\text{,} \qquad
  \widehat{\mathbf{\Sigma}}_x = \frac{1}{T-1}\sum_{t=1}^{T}\left(\mathbf{x}_t - \widehat{\boldsymbol{\mu}}_x\right)
  \left(\mathbf{x}_t - \widehat{\boldsymbol{\mu}}_x\right)^\intercal\text{.}
\label{eq:multivariatna-sredina-kovarijacijska}
\end{equation}

Ako pretpostavimo da slučajni vektor $\mathbf{X}$ dolazi is multivariatne normalne distribucije s
vektorom sredina $\boldsymbol{\mu}$ i kovarijaciskom matricom $\mathbf{\Sigma}$, tada je funkcija
izglednosti uzorka \engl{ likelihood function} dana jednadžbom \cite{statlect-likelihood}:
\begin{equation}
  \mathcal{L}\left(\boldsymbol{\mu}, \mathbf{\Sigma}; \mathbf{x_1}, \dots, \mathbf{x_T}\right) =
  \left(2\pi\right)^{-\frac{pT}{2}}|\det\left(\mathbf{\Sigma}\right)|^{-\frac{T}{2}}
  \exp\left(-\frac{1}{2}\sum_{t=1}^{T}\left(\mathbf{x}_t - \boldsymbol{\mu}\right)^\intercal
  \mathbf{\Sigma}^{-1}\left(\mathbf{x}_t - \boldsymbol{\mu}\right)\right)\text{.}
\end{equation}
S obzirom da taj oblik funckije izglednosti nije najprikladniji za računanje računalom iskazat ćemo i
njezin logaritamski oblik \engl{ log-likelihood function} te ćemo zamijeniti argument $\mathbf{\Sigma}$
s $\mathbf{\Sigma}^{-1}$:
\begin{equation} \label{eq:log-likelihood}
\begin{split}
  \ln\mathcal{L}&\left(\boldsymbol{\mu}, \mathbf{\Sigma}^{-1}; \mathbf{x_1}, \dots, \mathbf{x_T}\right) =\\
  &= \frac{1}{2}\left(
    -pT\ln(2\pi) + T\ln[\det(\mathbf{\Sigma}^{-1})] - \sum_{t=1}^{T}\left(\mathbf{x}_t - \boldsymbol{\mu}\right)^\intercal
    \mathbf{\Sigma}^{-1}\left(\mathbf{x}_t - \boldsymbol{\mu}\right)
  \right)\text{.}
\end{split}
\end{equation}


\section{Faktorski modeli}
\label{sec:faktorski}

U analizi povrata imovina često se primjenjuju multivarijatne statističke metode
kako bi se proučilo ponašanje i međusobna povezanost većeg broja vrijednosnica unutar portfelja.
Međutim, modeliranje velikog broja vremenskih nizova povrata dovodi do visokodimenzionalnih i
kompleksnih modela koji su složeni za interpretaciju i primjenu u praksi. Empirijska istraživanja
pokazuju da se povrati različitih vrijednosnica često kreću na sličan način, što nas upućuje da
postoje neki zajednički faktori koji utječu na njihovo kretanje. Primjerice, u razdobljima gospodarske
krize pad aktivnosti cijelog gospodarstva obično rezultira istodobnim padom cijena većine vrijednosnica,
dok rast određenog sektora, poput tehnološkog, često dovodi do rasta cijena većine dionica unutar
tog sektora \cite{anafin03}.

Te činjenice omogućuju faktorskim modelima da objasne kretanje većeg broja povrata pomoću
ogarničenog broja zajedničkih faktora te da pojednostavne njihovu analizu. Postoje tri vrste faktorskih
modela \cite{campbell1997}. Prvi su \emph{markoekonomski faktorski modeli} koji se fokusiraju na varijable
kao što su rast BDP-a, kamatne stope, stopa inflacije i slično. U ovakvom su modelu faktori osmotrivi
pa se model može estimirati linearnom regresijom. Drugi su \emph{fundamentalni faktorski modeli} koji
koriste podatke o poduzećima kako bi konsturiali svoje faktore. Treći su \emph{statistički faktorski
modeli} čiji su faktori neosmotrive latentne varijable koje se estimiraju iz podataka \cite{tsay2010}.
Ovaj rad će se fokusirati prvu vrstu odnosno na \emph{makroekonomske faktorske modele}.


\subsection{Linearni faktorski model}
\label{subsec:linearni-faktorski}

Pretpostavimo da imamo $p$ imovina kroz $T$ vremenskih trenutaka. Neka je $r_{it}$ povrat imovine $i$
u trenutku $t$. Opća forma faktorskog modela dana je izrazom:
\begin{equation}
  r_{it} = \alpha_i + \beta_{i1}f_{1t} + \dots + \beta_{im}f_{mt} + \epsilon_{it},
  \quad t=1,\dots,T; \quad i=1,\dots,p,
\label{eq:faktorski-opca}
\end{equation}
gdje je $\alpha_i$ konstanta, $\left\{f_{jt}|j=1,\dots,m\right\}$ su $m$ zajdeničkih (sistemskih)
faktora, $\beta_{ij}$ su koeficijenti imovine $i$ uz faktor $j$, a $\epsilon_{it}$ je specifičan
(idiosinkratski) faktor imovine $i$.

Za faktor $\mathbf{f}_t = \left(f_{1t}, \dots, f_{mt}\right)^\intercal$ vrijedi:
\begin{align}
  E(\mathbf{f}_t) &= \boldsymbol{\mu}_f, \\
  \mathrm{Cov}(\mathbf{f}_t) &= \mathbf{\Sigma}_f, \quad \text{$m \times m$ matrica,}
\end{align}
dok je idiosinkratski faktor $\epsilon_{it}$ modeliran bijelim šumom koji nije koreliran sa
sistemskim faktorima $f_{jt}$ i drugim idiosinkratskim faktorima. Dakle predpostavljamo:
\begin{align}
  \mathrm{E}(\epsilon_{it}) &= 0 \quad \text{za sve } i,t \\
  \mathrm{Cov}(f_{jt}, \epsilon_{is}) &= 0 \quad \text{za sve } i,j,t,s \\
  \mathrm{Cov}(\epsilon_{it}, \epsilon_{js}) &=
  \begin{cases}
    \sigma_i^2, &\text{ako $i = j$ i $t = s$}, \\
    0, &\text{inače.}
  \end{cases}
\end{align}

Jednadžbu (\ref{eq:faktorski-opca}) možemo zapisati i u matričnom obliku za svih $p$ imovina
u trenutku $t$:
\begin{equation}
  \mathbf{r}_t = \boldsymbol{\alpha} + \boldsymbol{\beta}\mathbf{f_t} + \boldsymbol{\epsilon_t},
  \quad t = 1, \dots, T,
\label{eq:faktorski-matricna}
\end{equation}
gdje je $\mathbf{r}_t = \left(r_{1t}, \dots, r_{pt}\right)^\intercal$ vektor povrata,
$\boldsymbol{\alpha} = \left(\alpha_1, \dots, \alpha_p\right)^\intercal$ vektor konstanti,
$\boldsymbol{\beta} = \left[\beta_{ij}\right]$ je $p \times m$ matrica koeficijenata, a
$\boldsymbol{\epsilon}_t = \left(\epsilon_{1t}, \dots, \epsilon_{pt}\right)^\intercal$ je
vektor idiosinkratskih faktora čija je kovarijacijska matrica
$\mathrm{Cov}(\boldsymbol{\epsilon}_t) = \boldsymbol{\Psi} = \mathrm{diag}\left\{
\sigma_1^2, \dots, \sigma_k^2\right\} \; p \times p$ dijagonalna matrica. Očekivanje i kovarijacisku
matricu povrata $\mathbf{r}_t$ možemo računati sljedećim formulama:
\begin{align}
  \mathrm{E}(\mathbf{r}_t) &= \boldsymbol{\alpha} + \boldsymbol{\beta}\boldsymbol{\mu}_f \label{eq:ocekivanje-r} \\
  \mathrm{Cov}(\mathbf{r}_t) &= \boldsymbol{\beta}\boldsymbol{\Sigma}_f\boldsymbol{\beta}^\intercal
  + \boldsymbol{\Psi}. \label{eq:kovarijacijska-r}
\end{align}

Zatim jednadžbu (\ref{eq:faktorski-matricna}) možemo zapisati u sljedećem obliku:
\begin{equation}
  \mathbf{r}_t = \boldsymbol{\xi}\mathbf{g}_t + \boldsymbol{\epsilon}_t,
\end{equation}
gdje $\mathbf{g}_t = \left(1, \mathbf{f}_t^\intercal\right)^\intercal$,
a $\boldsymbol{\xi} = [\boldsymbol{\alpha}, \boldsymbol{\beta}]$ je
$p \times (m+1)$ matrica. Transponiramo li prethodnu jednadžbu i grupiramo podatke za svih $T$ trenutaka
dobivamo:
\begin{equation}
  \mathbf{R} = \mathbf{G}\boldsymbol{\xi}^\intercal + \mathbf{E},
\label{eq:faktorski-matricna-ukupna}
\end{equation}
gdje je $\mathbf{R} \; T \times p$ matrica povrata čiji je $t$-ti redak $\mathbf{r}_t^\intercal$,
$\mathbf{G}$ je $T \times (m+1)$ matrica čiji je $t$-ti redak $\mathbf{g}_t^\intercal$,
$\mathbf{E}$ je $T \times p$ matrica idiosinkratskih faktora čiji je $t$-ti
redak $\boldsymbol{\epsilon}_t^\intercal$ \cite{tsay2010}.

S obzirom da nas zanimaju \emph{makroekonomski faktorski modeli} čiji su faktori $\mathbf{f}_t$
osmotrivi, jednadžba (\ref{eq:faktorski-matricna-ukupna}) ima oblik višestruke multivariatne
linearne regresije. Zbog toga parametre modela možemo estimirati metodom najmanjih kvadrata
\engl{ ordinary least squares, OLS} \cite{johnson2002}:
\begin{equation}
    \widehat{\boldsymbol{\xi}^\intercal} = \begin{bmatrix}
      \;\widehat{\boldsymbol{\alpha}}^\intercal\; \\
      \;\widehat{\boldsymbol{\beta}}^\intercal\;
    \end{bmatrix} = (\mathbf{G}^\intercal\mathbf{G})^{-1}(\mathbf{G}^\intercal\mathbf{R}),
\label{eq:ols-matrix}
\end{equation}
odakle su $\boldsymbol{\alpha}$ i $\boldsymbol{\beta}$ lako dostupni. Reziduale, odnosno
povrate idiosinkratskih faktora možemo dobiti koristeći formulu (\ref{eq:faktorski-matricna-ukupna}):
\begin{equation}
  \widehat{\mathbf{E}} = \mathbf{R} - \mathbf{G}\widehat{\boldsymbol{\xi}}^\intercal.
\label{eq:procjena-reziduala}
\end{equation}

\subsection{Jednofaktorski model povrata}
\label{subsec:jednofaktorski}

Jedan od najpoznatijih \emph{makroekonomskih faktorskih modela} koristi povrat tržišta
kao faktor koji utječe na sve vrijednosnice:
\begin{equation}
  r_{it} = \alpha_i + \beta_ir_{mt} + \epsilon_{it} \qquad i=1, \dots,p; \qquad t=1, \dots, T,
\label{eq:jednofaktorski}
\end{equation}
gdje je $r_{it}$ povrat vrijednosnice $i$ iznad bezrizične kamatne stope, a $r_{mt}$ povrat
tržišta iznad bezrizične kamatne stope. Kod modeliranja dionica, za povrat tržišta $r_{mt}$
uzima se povrat nekog tržišnog indeksa (npr. S\&P 500) iznad bezrizične
kamatne stope. Koeficijenti modela $\alpha_i$ i $\beta_i$ procjenjuju se metodom najmanjih
kvadrata (\ref{eq:ols-matrix}).

Ovaj rad fokusirat će se upravo na jednofaktorski model povrata. Cilj ovog rada biti će
ispitati može li model dubokog učenja, iz prozora povijesnih povrata
$\mathbf{R}_H=\left\{\mathbf{r}_t\right\}_{t=1}^k$,
procijeniti koeficijente $\alpha_i$ i $\beta_i$, koji će bolje odgovarati budućem prozoru povrata
$\mathbf{R}_F=\left\{\mathbf{r}_t\right\}_{t=k+1}^T$,
nego procjena koeficijenata $\alpha_i$ i $\beta_i$ koju možemo dobiti metodom najmanjih kvadrata
na istom povijesnom prozoru povrata $\mathbf{R}_H$ za $1\le k < T$. Način na koji ćemo mjeriti
koliko dobro procjena koeficijenata $\alpha_i$ i $\beta_i$ odgovara budućem prozoru povrata
$\mathbf{R}_F$ biti će detaljnije objašnjen u potpoglavlju \ref{sec:model-dubokog-ucenja}.


\section{Duboko učenje}
\label{sec:duboko-ucenje}

Duboko učenje predstavlja podpodručje strojnog učenja koje se ističe u rješavanju problema s
visokom dimenzionalnošću podataka, kao što su računalni vid, obrada prirodnog jezika,
financijska te slične složene domene. Temeljna ideja dubokog učenja je izgradnja hijerarhijskih,
složenih reprezentacija podataka, koje se dobivaju primjenom uzastopnih nelinearnih transformacija
modeliranih pomoću višeslojnih neuronskih mreža. Osnovni element svake neuronske mreže je umjetni neuron.
Za zadani ulazni vektor $\mathbf{x} = (x_1, \dots, x_n)^\intercal$, izlaz neurona definiramo jednadžbom:
\begin{equation}
  h = f\left(\mathbf{w}^\intercal\mathbf{x} + b\right)
\end{equation}
gdje je $\mathbf{w} = (w_1, \dots, w_n)^\intercal$ vektor težina neurona koji odrežuje doprinos
pojedine komponente ulaznog vektora, a skalar $b$ omogućuje dodatni pomak linearne kombinacije ulaza.
Aktivacijska funkcija $f$ uvodi nelinearnost u model, čime se omogućuje aproskimacija složenih i
nelinearnih odnosa u podatcima \cite{du02}.

Neke od najčešće korištenih aktivacijskih funkcija su sigmoidalna funkcija i hiperbolni tangens,
slika \ref{fig:activations}.
Obje su nelinearne, monotono rastuće i kontinuirano diferencijabilne, što je ključno svojstvo u
procesu učenja neuronskih mreža metodama koje se temelje na gradijentnom spustu. Sigmoidalna funkcija
ima kodomenu u intervalu $\left\langle0,1\right\rangle$, dok hiperbolni tangens poprima vrijednosti u
intervalu $\left\langle-1,1\right\rangle$, pri čemu obje funckije imaju karakterističan S-oblik.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/activation_functions.pdf}
\caption{Primjer aktivacijskih funkcija}
\label{fig:activations}
\end{figure}

Umjetne neurone zatim možemo organizirati u slojeve. Prvi, odnosno ulazni, sloj, zatim proizvoljan broj skrivenih
te na kraju izlazni sloj. Time dobivamo osnovnu arhitekturu neuronske mreže, slika \ref{fig:neural-net}.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/neural-net.pdf}
\caption{Jednostavna neuronska mreža}
\label{fig:neural-net}
\end{figure}

Među najznačajnijim arhitekturama dubokih modela ističu se duboke unaprijende mreže, konvolucijske
neuronske mreže te povratne neuronske mreže. Svaka od navedenih arhitektura prilagođena je specifičnim 
vrstama podataka i problemima. Odabir odgovarajuće arhitekture ovisi o prirodi i strukturi odabranih podataka. 

\subsection{Povratne neuronske mreže}
\label{subsec:povratne-mreze}

Dijeljenje parametara jedna je od ranijih ideja u strojnom učenju i statističkom modeliranju, koja
omogućuje proširivanje modela na primjere različitih oblika te generalizaciju na podatke varijabilne
duljine, osobito u kontekstu sekvencijalnih podataka. Ovakav se princip koristi u skrivenim
Markovljevim modelima, gdje se isti skup parametara, poput matrice prijelaza
$P(\mathbf{s}_t \mid \mathbf{s}_{t-1})$, koristi u svakom vremenskom koraku. Na taj način model ne ovisi
o apsolutnoj poziciji u sekvenci, već isključivo o odnosu između susjednih stanja, što omogućuje
modeliranje sekvenci proizvoljne duljine  i dijeljenje statističke snage kroz različite vremenske korake.
Povratne neuronske mreže \engl{ recurrent neural networks, RNN} preuzimaju i generaliziraju ovu ideju u
okviru dubokog učenja. One su prilagođene za obradu sekvencijalnih podataka tako da u svakom vremenskom
koraku primjenjuju iste težine i istu transformaciju nad ulazom i prethodnim skrivenim stanjem, pri čemu
skriveno stanje $\mathbf{h}_t$ ovisi o trenutnom ulazu $\mathbf{x}_t$ i stanju $\mathbf{h}_{t-1}$
\cite{goodfellow2016}.


\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/RNN-unrolled.png}
\caption{Jednostavna povratna neuronska mreža \cite{colah2015}}
\label{fig:unrolled-rnn}
\end{figure}

Slika \ref{fig:unrolled-rnn} prikazuje dijagram jednostavne povratne neuronske mreže i njezin razmotani
\engl{ unrolled} oblik kroz vremenske korake. Povratnu neuronsku mrežu moguće je promatrati kao višestruke
kopije iste neuronske mreže, pri čemu svaka kopija prosljeđuje informaciju svojoj sljedbenici u idućem
vremenskom koraku. U razmotanom prikazu jasno se vidi da se u svakom koraku primjenjuje ista RNN ćelija 
označena slovom A s istim parametrima u svakom koraku, dok skriveno stanje predstavlja viđeni dio slijeda
koja se prenosi kroz vrijeme. Na taj način model zadržava informaciju o prethodnim ulazima te postupno
akumulira kontekst, što mu omogućuje modeliranje ovisnosti unutar sekvence, neovisno o njezinoj
duljini \cite{colah2015}.

Jednostavni povratni model možemo definirati sljedećim jednadžbama:
\begin{align}
  \mathbf{h}_t &= \tanh(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_{t} + \mathbf{b}_h), \\
  \mathbf{o}_t &= \mathbf{W}_{hy}\mathbf{h}_t + \mathbf{b}_o,
\end{align}
gdje matrica $\mathbf{W}_{xh}$ projicira ulaz u prostor reprezentacije stanja, matrica $\mathbf{W}_{hh}$
modelira evoluciju stanja dok matrica $\mathbf{W}_{hy}$ projicira stanje na prostor predikcija. Vektori
$\mathbf{b}_h$ i $\mathbf{b}_o$ omogućuju linearni pomak \cite{du05}.

Međutim, pri učenju dugoročnih zavisnosti u povratnim neuronskim mrežama pojavljuju se značajni
matematički problemi. Gradijenti koji se propagiraju kroz velik broj vremenskih koraka imaju tendenciju
eksponencijalnog smanjivanja (problem nestajućeg gradijenta) ili, rjeđe, eksponencijalnog rasta
(problem eksplodirajućeg gradijenta), što otežava stabilnu optimizaciju modela. Čak i ako pretpostavimo
da su parametri stabilni, utjecaj udaljenih vremenskih koraka postaje eksponencijalno manji u odnosu
na nedavne, zbog čega mreža teško uči dugoročne ovisnosti \cite{goodfellow2016}.


\subsection{LSTM mreže}
\label{subsec:lstm-mreze}

U okviru ovog rada koristit ćemo posebnu vrstu povratnih neuronskih mreža koje nazivamo povratna ćelija s
dugoročnom memorijom \engl{ long short-term memory, LSTM}. LSTM mreže dizajnirane su kako bi
izbjegle probleme nestajućeg i eksplodirajućeg gradijenta te omogućile učenje dugoročnih ovisnosti
u sekvencijalnim podatcima. Zbog toga se često primjenjuju u modeliranju vremenskih nizova \cite{colah2015}.

Razmotana LSTM mreža ima strukturu lančanih ćelija karakterističnu za povratne neuronske mreže, no njene
ponavljajuće ćelije imaju složeniju unutarnju arhitekturu. Umjesto jednog neuronskog sloja, LSTM ćelija
sastoji se od četiri sloja koji imaju specifičnu interakciju.
\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/LSTM-chain.png}
\caption{Arhitektura LSTM mreže \cite{colah2015}}
\label{fig:lstm-architecture}
\end{figure}
Ključni element LSTM arhitekture jest stanje ćelije $\mathbf{C}_t$, koje predstavlja internu memorijsku
komponentu modela. Ono se propagira kroz vremenske korake koristeći samo linearne transformacije,
čime se omogućuje stabilniji prijenos informacija i ublažava problem nestajućeg gradijenta.
\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{img/LSTM-C-line.png}
\caption{Tok stanja ćelije $\mathbf{C}_t$ \cite{colah2015}}
\label{fig:lstm-c-line}
\end{figure}
Za razliku od standardnog skrivenog stanja u jednostavnim RNN mrežama, stanje ćelije eksplicitno
je regulirano posebnim mehanizmima vrata \engl{ gate}. LSTM arhitektura koristi tri vrste vrata: vrata
zaboravljanja, ulazna vrata te izlazna vrata.


\subsubsection{Vrata zaboravljanja}
\label{subsubsec:vrata-zaboravljanja}

Prvi korak u LSTM mreži se odnosi se na određivanje koje informacije iz prethodnog stanja ćelije
$\mathbf{C}_{t-1}$ treba izbaciti. Ta se odluka donosi pomoću vrata zaboravljanja, koja kao ulaz primaju
prethodno skriveno stanje $\mathbf{h}_{t-1}$ i trenutačni ulaz $\mathbf{x}_t$. Rezultat je vektor 
$\mathbf{f}_t$ koji računamo na sljedeći način:
\begin{equation}
  \mathbf{f}_t = \sigma\left(\mathbf{W}_f\cdot\left[\mathbf{h}_{t-1},\mathbf{x}_t\right] + \mathbf{b}_f\right).
\label{eq:forget-gate}
\end{equation}
Pošto vrata zaboravljanja koriste sigmoidu kao aktivacijsku funkciju, $\mathbf{f}_t\in[0,1]^n$,
elementi stanja $\mathbf{C}_t$ na pozicijama na kojima je vektor $\mathbf{f}_t$ blizu nuli bit
će zaboravljeni, dok će se oni na pozicijama na kojima je vektor $\mathbf{f}_t$ blizu jedinici
propustiti u iduće stanje.

\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{img/LSTM-forget-gate.png}
\caption{Vrata zaboravljanja \cite{colah2015}}
\label{fig:lstm-forget-gate}
\end{figure}


\subsubsection{Ulazna vrata}
\label{subsubsec:ulazna-vrata}

U idućem koraku moramo odrediti koju ćemo informaciju iz ulaza $\mathbf{x}_t$ dodati stanju ćelije.
Ovo radimo u dva koraka:
\begin{align}
  \mathbf{i}_t &= \sigma\left(\mathbf{W}_f\cdot\left[\mathbf{h}_{t-1},\mathbf{x}_t\right] + \mathbf{b}_i\right), \\
  \widetilde{\mathbf{C}}_t &= \tanh\left(\mathbf{W}_C\cdot\left[\mathbf{h}_{t-1},\mathbf{x}_t\right] + \mathbf{b}_C\right).
\end{align}
Vektor $\mathbf{i}_t$ određuje koje informacije je potrebno ažurirati, a sloj koji koristi
tangens hiperbolni računa doprinose $\widetilde{\mathbf{C}}_t$ iz ulaza $\mathbf{x}_t$ koje
ćemo dodati stanju ćelije $\mathbf{C}_{t-1}$.

\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{img/LSTM-input-gate.png}
\caption{Ulazna vrata \cite{colah2015}}
\label{fig:lstm-input-gate}
\end{figure}


\subsubsection{Ažuriranje stanja ćelije}
\label{subsubsec:azuriranje-celije}

Zatim stanje ćelije zatim ažuriramo na sljedeći način:
\begin{equation}
  \mathbf{C}_t = \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \widetilde{\mathbf{C}}_t.
\end{equation}
gdje je $\odot$ oznaka za Hadamartov umnožak. Množimo staro stanje s $\mathbf{f}_t$ kako bi zaboravili
podatke koje smo odredili ranije. Zatim dodamo umnožak $\mathbf{i}_t \odot \widetilde{\mathbf{C}}_t$
koji predstavlja odabrane doprinose ulaza.

\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{img/LSTM-update-state.png}
\caption{Ažuriranje stanja ćelije \cite{colah2015}}
\label{fig:lstm-update-state}
\end{figure}


\subsubsection{Izlazna vrata}
\label{subsubsec:izlazna-vrata}

Na kraju, izlaz ćelije će se temeljiti na novom stanju ćelije $\mathbf{C}_t$. Prvo ćemo pomoću
izlaznih vrata $\mathbf{o}_t$ odrediti koje djelove stanja želimo zadržati na izlazu.
Potom ćemo stanje ćelije prvo propustit kroz tangens hiperbolni prije nego ga pomnožimo s
izlaznim vratima:
\begin{align}
  \mathbf{o}_t &= \sigma\left(\mathbf{W}_o\cdot\left[\mathbf{h}_{t-1},\mathbf{x}_t\right] + \mathbf{b}_o\right), \\
  \mathbf{h}_t &= \mathbf{o}_t \odot \tanh\left(\mathbf{C}_t\right)
\end{align}

\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{img/LSTM-out-gate.png}
\caption{Izlazna vrata \cite{colah2015}}
\label{fig:lstm-out-gate}
\end{figure}


\subsection{Trenirnanje modela dubokog učenja}
\label{subsec:treniranje-modela}

Svaki algoritam strojnog učenja možemo rastaviti na tri glavne komponente \cite{su1osnovni}. Prvu komponentu,
odnosno model, možemo definirati kao skup funckija koje su parametrizirane vektorom parametara
$\boldsymbol{\theta}$:
\begin{equation}
  \mathcal{H} = \{h(\mathbf{x};\boldsymbol{\theta})\}_{\boldsymbol{\theta}}.
\end{equation}

Druga komponenta je funkcija gubitka $L$ \engl{ loss function} s pripadnom funkcijom pogreške:
\begin{equation}
  E(\boldsymbol{\theta}|\mathcal{D}) = \frac{1}{N} \sum_{i=1}^{N}L(y^{(i)}, h(\mathbf{x}^{(i)};\boldsymbol{\theta})),
\end{equation}
gdje $\mathcal{D}$ predstavlja skup označenih primjera, a $y^{(i)}$ točnu oznaku primjera $\mathbf{x}^{(i)}$.
Funkcija gubitka za dani vektor parametara $\boldsymbol{\theta}$ govori koliko se izlaz modela razlikuje
od željenog izlaza.

Treća komponenta je optimizacijski postupak:
\begin{equation}
  \boldsymbol{\theta}^* = \argmin_{\boldsymbol{\theta}} E(\boldsymbol{\theta}|\mathcal{D}).
\end{equation}
Cilj optimizacijskog postupka je pronalazak vektora parametara $\boldsymbol{\theta}^*$ koji minimiziraju
funkciju pogreške. Vektor parametara $\boldsymbol{\theta}^*$ jednoznačno određuje funckiju $h^*$.

Jedan konkretan optimizacijski postupak koji se često koristi kod modela dubokog učenja je gradijentni
spust. Ideja gradijentnog spusta je da se, krenuvši od nasumično incijaliziranog vektora parametara
$\boldsymbol{\theta}$, postepeno spuštamo niz površinu funkcije pogreške $E(\boldsymbol{\theta}|\mathcal{D})$
u smjeru suprotnome od gradijenta u točki $\boldsymbol{\theta}$. To ponavljamo dok se ne spustimo u
točku u kojoj je gradijent jednak ili jako blizu nuli. Pri tome vektor parametara $\boldsymbol{\theta}$
ažuriramo u vakoj iteraciji na sljedeći način:
\begin{equation}
  \boldsymbol{\theta} = \boldsymbol{\theta} - \eta \nabla E(\boldsymbol{\theta}|\mathcal{D})
\end{equation}
gdje je $\eta$ stopa učenja koja određuje veličinu koraka koje radimo spuštajući se prema minimumu
\cite{su1logisticka}.

Međutim fokusiramo li se isključivo na minimiziraju funkciju pogreške pronaći ćemo vektor parametara
koji je previše prilagođen na podatke i koji neće dobro raditi na podatcima koji nisu bili u skupu za
treniranje. Želimo pronaći vektor parametara koji će raditi dobro i za neviđene podatke odnosno koji će
dobro generalizirati. Kako bi to postigli, podijelit ćemo skup primjera na skup za treniranje, validaciju
i testiranje $\mathcal{D} = \mathcal{D}_{train} \cup \mathcal{D}_{validation} \cup \mathcal{D}_{test}$.
Potupak optimizacije zatim ćemo provoditi samo na skupu za treniranje $\mathcal{D}_{train}$.
Nakon što smo napravili podjelu računamo dvije pogreške. Prva je pogreška funckije $h$ na skupu za učenje
$\mathcal{D}_{train}$, tj. $E(h|\mathcal{D}_{train})$, a druga pogreška funkcije $h$ na skupu za validaciju
$E(h|\mathcal{D}_{validation})$. Postupak optimizacije zaustavit ćemo nakon što smo dosegnuli minimum pogreške
na skupu za validaciju. Nakon toga računamo pogrešku na skupu za testiranje $E(h|\mathcal{D}_{test})$ te će
nam ta vrijednost služiti kao mjera performansi dobivene funkcije $h$. Također pogrešku na skupu za
testiranje možemo koristiti kako bi objektivno uspoređivali modele.

%-------------------------------------------------------------------------------
\chapter{Implementacija}
\label{pog:implementacija}

U ovom poglavlju opisana je programska implementacija predloženog rješenja te alati i biblioteke koje su
omogućile izgradnju, treniranje i evaluaciju modela. Implementacija je realizirana u programskom
jeziku Python 3 zbog njegove široke primjene u području strojnog učenja i bogatog ekosustava znanstvenih
biblioteka. U tablici \ref{tab:biblioteke} su navedene glavne biblioteke korištene u radu, zajedno s
njihovom ulogom u implementaciji.

\begin{table}[h]
\centering
\caption{Korištene Python biblioteke}
\label{tab:biblioteke}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lp{10cm}} \toprule
\textbf{Biblioteka} & \textbf{Namjena} \\ \midrule
\texttt{hydra-core} & Upravljanje konfiguracijama i pojednostavljenje podešavanja hiperparametara modela \cite{hydra} \\
\texttt{lightning}  & Pojednostavljenje i ubrzavanje procesa treniranja modela \cite{lightning} \\
\texttt{matplotlib} & Vizualizacija podataka i rezultata \cite{matplotlib} \\
\texttt{numpy}      & Rukovanje višedimenzionalnim poljima podataka \cite{numpy} \\
\texttt{pandas}     & Učitavanje i obrada tabličnih podataka \cite{pandas} \\
\texttt{scipy}      & Estimacija parametara distribucija i generiranje sintetičkih podataka \cite{scipy} \\
\texttt{torch}      & Razvoj i treniranje modela strojnog učenja \cite{torch} \\ \bottomrule
\end{tabular}
\end{table}


\section{Podatci}
\label{sec:podatci}

Model implementiran u ovom radu temelji se na jednofaktorskom modelu povrata. Budući da se u modelu
procjenjuju koeficijenti $\alpha_i$ i $\beta_i$, na ulazu su potrebni povrati tržišta i dionica iznad
bezrizične kamatne stope. S obzirom da će se ovaj rad fokusirati na dnevne povrate, razlika između
artimetičkih i logaritamskih je gotovo zanemariva. Stoga biramo dvenve artimetičke povrate za ovaj rad.
Svi korišteni podatci preuzeti su iz baze podataka dostupne na mrežnim stranicama
Kenetha R. Frencha \cite{fammafrench}.


\subsubsection{Tržišni povrati}
\label{subsubsec:trzisni-povrati}

Podatci o dnevnim artimetičkim tržišnim povratima i denvneoj bezrizičnoj kamatnoj stopi preuzeti su iz
skupa podataka \emph{Fama/French 3 Factors [Daily]}.

Tržišni povrat definiran je kao vrijednosno otežani \engl{ value weighted} povrat svih poduzeća iz baze
CRSP inkorporiranih u SAD-u i izlistanih na burzama NYSE, AMEX ili NASDAQ. Takva konstrukcija tržišnog
portfelja po svojoj strukturi i obuhvatu vrlo je slična indeksu koji prati SPDR S\&P 500 ETF, budući da
oba predstavljaju široko diverzificirani presjek američkog tržišta kapitala.

Bezrizična kamatna stopa definirana je kao prinos na jednomjesečni američki trezorski zapis. Do svibnja
2024. godine podatci potječu od Ibbotson Associates, dok se od lipnja 2024. godine koristi referentna stopa
iz ICE BofA US 1-Month Treasury Bill Indexa.


\subsubsection{Povrati dionica}
\label{subsubsec:povrati-dionica}

Podatci o dnevnim artimetičkim povratima dionica preuzeti su iz skupa \emph{25 Portfolios Formed on Size and
Book-to-Market (5 $\times$ 5) [Daily]}. Riječ je o 25 portfelja formiranih kao presjek pet portfelja
prema veličini poduzeća \engl{market equity, ME} i pet portfelja prema omjeru knjigovodstvene i tržišne
vrijednosti kapitala \engl{ book-to-market equity, BE/ME}. Portfelji se konstruiraju na kraju lipnja
svake godine kao presjeci:
\begin{itemize}
  \item pet kvintila prema tržišnoj kapitalizaciji ME
  \item pet kvintila prema omjeru BE/ME
\end{itemize}

U ovom radu koriste se povrati navedenih portfelja, a ne pojedinačnih dionica. Razlog tome je
što pojedinačne dionice tijekom vremena ulaze i izlaze s tržišta, što može uzrokovati probleme u
konstrukciji uravnoteženih vremenskih nizova.


\section{Obrada podataka}
\label{sec:obrada-podataka}

U ovom potpoglavlju ćemo opisati kako od podataka navedenih u potpoglavlju \ref{sec:podatci} konstruirati
značajke za naš model. Prvo nakon što smo učitali podatke o artimetičkim povratima tržišta, portfelja i
bezrizičnu kamatnu stopu, uklanjamo nedostajuće podatke. U našim podatcima su to retci u kojima se nalaze
vrijednosti -99.99 ili -999. Podatke zatim filtriramo na raspon od 1. 1. 1950. do 28. 11. 2025. uključivo.
Odbacujemo podatke prije 1950. godine jer je presjek prvog kvintila po tržišnoj kapitalizaciji i prvog
kvintila po omjeru BE/ME bio jako nestabilan\footnote{U tim periodima se taj presjek sastojao od svega
nekoliko dionica}. Preuzeti podatci izraženi su u postotcima umjesto u skalarnim vrijednostima stoga ćemo
sve podatke podijeliti sa 100.

S obzirom da je od povrata tržišta već oduzeta bezrizična kamatna stopa, još nam preostaje oduzeti bezrizičnu
kamatnu stopu od povrata portfelja kako bi dobili povrate iznad bezrizične kamatne stope. Neka je $R_{Mt}$
dnevni artimetički povrata iznad bezrizične kamatne stope tržišta u trenutku $t$, a $R_{it}$ dnevni
artimetički povrata iznad bezrizične kamatne stope portfelja $i$ u trenutku $t$. Definirajmo zatim
vektore $\mathbf{R}_M = \left(R_{M1}, \dots R_{MT}\right)^\intercal$ i $\mathbf{R}_i = \left(R_{i1},
\dots, R_{iT}\right)^\intercal$ te matricu $\mathbf{R}_{P25} = [\mathbf{R}_1, \dots, \mathbf{R}_{25}]$
oblika $T \times 25$, gdje je $T$ ukupan broj dnevnih povrata u našim podatcima.

U ovom radu odlučili smo se uzorkovati podatke kliznim prozorom duljine 90. Prozore ćemo zatim
podijeliti tako da prvih 60 povrata smatramo povijesnim povratima, a posljednjih 30 budućim povratima.
Za duljinu koraka kliznog prozora uzeli smo 30 kao kompromis između preklapanja prozora i broja uzorka.
S obzirom da je duljina koraka jednaka duljini prozora budućih povrata, osigurali smo da se barem
vrijednosti u prozoru budućih povrata ne preklapaju.
Uzorkovanje vektora $\mathbf{R}_M$ i matrice $\mathbf{R}_{P25}$ postižemo na sljedeći način:
\begin{align}
  \mathbf{w}_{Mk} &= (R_{Mi}, \dots, R_{M(i+89)})^\intercal &i=30k+1; \quad 0 \le k \le \left\lfloor\frac{T-90}{30}\right\rfloor, k \in \mathbb{Z}  \\
  \mathbf{W}_{Pk} &= \begin{bmatrix}
      \; \mathbf{R}_{P25}^{(i)} \; \\
      \; \vdots \; \\
      \; \mathbf{R}_{P25}^{(i+89)} \;
    \end{bmatrix} &i=30k+1; \quad 0 \le k \le \left\lfloor\frac{T-90}{30}\right\rfloor, k \in \mathbb{Z}
\end{align}
gdje je $\mathbf{R}_{P25}^{(i)}$ $i$-ti redak matrice $\mathbf{R}_{P25}$. Dobivene prozore zatim dijelimo na
način da prvih 60 povrata prozora uzimamo kao povijesne povrate,a posljednjih 30 povrata kao buduće:
\begin{align}
  \mathbf{x}_{Mk} &= (w_{Mk1}, \dots, w_{Mk60})^\intercal & \mathbf{y}_{Mk} &= (w_{Mk61}, \dots, w_{Mk90})^\intercal, \\
  \mathbf{X}_{Pk} &= \begin{bmatrix}
      \; \mathbf{W}_{Pk}^{(1)} \; \\
      \; \vdots \; \\
      \; \mathbf{W}_{Pk}^{(60)} \;
    \end{bmatrix} & \mathbf{Y}_{Pk} &= \begin{bmatrix}
      \; \mathbf{W}_{Pk}^{(61)} \; \\
      \; \vdots \; \\
      \; \mathbf{W}_{Pk}^{(90)} \;
    \end{bmatrix},
\end{align}
gdje $w_{Mki}$ $i$-ti element vektora $\mathbf{w}_{Mk}$.

Prije nego što povijesni prozori $\mathbf{x}_{Mk}$ i $\mathbf{X}_{Pk}$ budu spremni za naš model, dodajemo još
jednu značajku u obliku interakcije:
\begin{align}
  \mathbf{X}_{Mk} &= \mathbf{x}_{Mk} \cdot \mathbf{1}_{25}^\intercal, \\
  \mathbf{X}_{PMk} &= \mathbf{X}_{Mk} \odot \mathbf{X}_{Pk},
\end{align}
gdje je $\mathbf{1}_n$ vektor jedinica dužine $n$. Neka je
$\mathbf{X}_k = [\mathbf{X}_{Pk}^\intercal, \mathbf{X}_{Mk}^\intercal, \mathbf{X}_{PMk}^\intercal]$ tenzor
oblika $25\times60\times3$. Time smo konstruirali $k$ ulaznih tenzora za naš model.


Jedna od funkcija cilja našeg modela je negativna logaritamska izglednost. Za njezin izračun potrebno nam je
očekivanje i kovarijacijska matrica povrata (\ref{eq:log-likelihood}). S obzirom da naš model pretpostavlja
jednofaktorski model povrata (\ref{eq:jednofaktorski}) očekivanje i kovarijacijsku matricu možemo izračunati
koristeći formule (\ref{eq:ocekivanje-r}) i (\ref{eq:kovarijacijska-r}). Naš model procjenjuje koeficijente
$\widehat{\boldsymbol{\alpha}}$ i $\widehat{\boldsymbol{\beta}}$, stoga kako bi mogli računati negativnu log
izglednost moramo još procijeniti $\mu_f$, $\sigma_f^2$ i $\boldsymbol{\Psi}$ iz prozora budućih povrata. S
obzirom da naš faktor predstavljaju povrati tržišta, $\widehat{\mu}_{fk}$ i $\widehat{\sigma}_{fk}^2$
procjenjujemo iz prozora budućih povrat tržišta $\mathbf{y}_{Mk}$ pomoću izraza (\ref{eq:normalna-sredina}) i
(\ref{eq:normalna-varijanca}). Kako bi izračunali procjenu kovarijacijske matrice reziduala
$\widehat{\boldsymbol{\Psi}}$ primjenjujemo formulu (\ref{eq:multivariatna-sredina-kovarijacijska}) na
rezidualima $\widehat{\mathbf{E}}$. Kako bi izračunali $\widehat{\mathbf{E}}$ možemo iskoristiti formulu
(\ref{eq:procjena-reziduala}). Za nju su nam potrebni parametri $\widehat{\boldsymbol{\xi}}^\intercal$ koje
možemo dobiti primjenom metode najmanjih kvadrata (\ref{eq:ols-matrix}) na $\mathbf{y}_{Mk}$ i
$\mathbf{Y}_{Pk}$. Nakon što smo dobili parametre $\widehat{\boldsymbol{\xi}}_k^\intercal$ možemo procijeniti
$\widehat{\boldsymbol{\Psi}}_k$. Skup $\mathbf{Y}_k = \left\{\mathbf{Y}_{Pk}, \mathbf{y}_{Mk},
\widehat{\mu}_{fk}, \widehat{\sigma}_{fk}^2, \widehat{\boldsymbol{\xi}}_k^\intercal,
\widehat{\boldsymbol{\Psi}}_k\right\}$ predstavlja ciljne varijable naših $k$ uzorka.


\section{Sintetički podatci}
\label{sec:sinteticki}

Osim stvarnih podataka naš model trenirali smo i na sintetičkim podatcima. Prednost sintetičkih podataka
je to što ih možemo generirti prozivljno mnogo te su vrijednosti parametara koje pokušavamo procijeniti
poznate. Kako bi sintetički podatci mogli pomoći modelu u generalizaciji na stvarne podatke moraju biti
što sličniji stvarnim podatcima.

Za simulaciju tržišnih povrata odabralismo studentovu $t$-distribuciju s pet stupnjeva slobode. Za procjenu
$\mu_M$ i $\sigma_M^2$ koristili smo procjenu najveće izglednosti koju nudi biblioteka \texttt{scipy} na
podatcima $\mathbf{R}_M$. Primjenjivanjem metode najmanjih kvadrata (\ref{eq:ols-matrix}) na
$\mathbf{W}_{Mk}$ i $\mathbf{W}_{Pk}$ dobivamo $k$ parametara $\widehat{\boldsymbol{\xi}}_k$.
To nam omogućuje da uz formulu (\ref{eq:procjena-reziduala}) dobijemo $k$ reziduala $\widehat{\mathbf{E}}_k$.
Iz parametra $\widehat{\boldsymbol{\xi}}_k$ lako su dostupni $\widehat{\boldsymbol{\alpha}}_k$ i
$\widehat{\boldsymbol{\beta}}_k$. Potom sve elemente $\widehat{\mathbf{E}}_k$,
$\widehat{\boldsymbol{\alpha}}_k$ i $\widehat{\boldsymbol{\beta}}_k$ promatramo kao realizacije triju
normalnih distribucija $\{\epsilon_i\}_{i=1}^{90\times25\times k}$, $\{\alpha_i\}_{i=1}^{25\times k}$ i
$\{\beta_i\}_{i=1}^{25\times k}$. Primjenom formula (\ref{eq:normalna-sredina}) i
(\ref{eq:normalna-varijanca}) dobivamo procjene $\widehat{\mu}_{\epsilon}$, $\widehat{\sigma}_{\epsilon}^2$,
$\widehat{\mu}_{\alpha}$, $\widehat{\sigma}_{\alpha}^2$, $\widehat{\mu}_{\beta}$ i
$\widehat{\sigma}_{\beta}^2$. Sintetičke podatke zatim generiramo kako je opisano u tablici
\ref{tbl:sinteticki}, gdje je $N$ željeni broj uzoraka, a $p$ broj simuliranih portfelja.
\begin{table}
\caption{Generiranje sintetičkih podataka}
\label{tbl:sinteticki}
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{ccl} \toprule
  \textbf{Vektor uzoraka} & \textbf{Distribucija} & \textbf{Broj uzoraka} \\ \midrule
  $\mathbf{R}_{MS}$ & $t_5(\widehat{\mu}_M, \widehat{\sigma}_M^2)$ & $N\times90$ \\
  $\boldsymbol{\epsilon}_S$ & $\mathcal{N}(\widehat{\mu}_{\epsilon}, \widehat{\sigma}_{\epsilon}^2)$ & $N\times90\times p$ \\
  $\boldsymbol{\alpha}_S$ & $\mathcal{N}(\widehat{\mu}_{\alpha}, \widehat{\sigma}_{\alpha}^2)$ & $p$ \\
  $\boldsymbol{\beta}_S$ & $\mathcal{N}(\widehat{\mu}_{\beta}, \widehat{\sigma}_{\beta}^2)$ & $p$ \\ \bottomrule
\end{tabular}
\end{table}
Nakon toga vektor $\boldsymbol{\epsilon}_S$ preoblikujemo u matricu $\mathbf{E}_S$ oblika
$(N\times90)\times p$. Generirani vektor $\mathbf{R}_{MS}$ predstavlja povrate tržišta $\mathbf{R}_{M}$,
a simulirane povrate portfelja $\mathbf{R}_{P25}$ dobivamo na sljedeći način:
\begin{align}
  \mathbf{A}_S &= \mathbf{1}_{N\times90} \cdot \boldsymbol{\alpha}_S^\intercal \quad \text{matrica $(N\times90)\times p$}, \\
  \mathbf{R}_{PS} &= \mathbf{A}_S + \mathbf{R}_{MS}\boldsymbol{\beta}_S^\intercal + \mathbf{E}_S
\end{align}
Nakon što imamo povrate tržišta $\mathbf{R}_{MS}$ i povrate portfelja $\mathbf{R}_{PS}$ možemo ponoviti korake
iz poglavlja \ref{sec:obrada-podataka} kako bi generiali ulazne tenzore i ciljne varijable. Postoje samo
dvije razlike. Kod sintetičkih podataka korak kliznog prozora postavljamo na 90 jer nema razloga da nam se
sintetički podatci preklapaju. Kod procjenu kovarijacijske matrice reziduala $\widehat{\boldsymbol{\Psi}}$
nema potrebe da procjenjujemo parametre $\widehat{\boldsymbol{\xi}}$ metodom najmanjih kvadrata jer su
nam oni već poznati $\boldsymbol{\xi} = \left[\boldsymbol{\alpha}_S,\: \boldsymbol{\beta}_S\right]$.

\section{Model dubokog učenja}
\label{sec:model-dubokog-ucenja}

Naš model sastoji se od enkodera i nekoliko različitih dekodera prilagođenih različitim funckijama
gubitka. S obzirom na takvu arhitekturu naš model možemo smatrati generativnim modelom.
Dovođenjem simuliranih podataka na dekoder zajedno s izlazom enkodera dobivene izlaze možemo koristiti
za testiranje različitih scenarija te za upravljanje rizikom.


\subsubsection{Enkoder}
\label{subsubsec:enkoder}

Glavna komponenta našeg modela je enkoder koji se sastoji od jedne LSTM jedinice i dva linearna
sloja. Neka je $\mathbf{X}_{ki}$ $i$-ta od $p$ matrica iz tenzora $\mathbf{X}_k$ gdje je $p$ broj
portfelja i prva dimenzija tenzora $\mathbf{X}_k$. Matrica $\mathbf{X}_{ki}$ ima oblik $60\times3$ i
sadrži 60 povrata $i$-tog portfelja, tržišta te njihov produkt za $k$-ti prozor podataka. Enkoder
na ulazu prima ta tri vremenska niza i prosljeđuje ih LSTM jedinici. Potom uzimamo posljednje skriveno
stanje LSTM jedinice $\mathbf{h}_{60}$ i stavljamo ga na ulaz dva zasebna linearna sloja. Zadaća linearnih
slojeva je procijeniti koeficijente $\alpha_i$ i $\beta_i$ jednofaktorskog modela koji najbolje odgovarati
budućem prozoru povrata iz posljednjeg skrivenog stanja $\mathbf{h}_{60}$. Slika \ref{fig:encoder}
prikazuje arhitekturu enkodera. Enkoderu također možemo primjeniti na čitav tenzor $\mathbf{X}_k$ čime
dobivamo vektore $\boldsymbol{\alpha}$ i $\boldsymbol{\beta}$, odnosno procjenu koeficijenata $\alpha_i$
i $\beta_i$ za svih $p$ portfelja.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/encoder.pdf}
\caption{Enkoder}
\label{fig:encoder}
\end{figure}


\subsubsection{Dekoderi i funckije gubitka}
\label{subsubsec:dekoderi}

U ovom radu ispitali smo tri različite funckije gubitka od kojih je svaka zahtjevala zaseban dekoder.
Prva funckija gubitka je srednja kvadratna pogreška \engl{ mean squared error, MSE}. Srednju kvadratnu
pogrešku našeg modela računat ćemo na sljedeći način:
\begin{equation}
  L_{MSE}\left(\widehat{\mathbf{Y}}_{Pk}, \mathbf{Y}_{Pk}\right) = \frac{1}{p\times30} \sum_{i=1}^{p}
  \sum_{j=1}^{30} \left(\mathbf{Y}_{Pk}^{(j,i)} - \widehat{\mathbf{Y}}_{Pk}^{(j,i)}\right)^2,
\end{equation}
gdje je $\mathbf{Y}_{Pk}^{(j,i)}$ element matrice $\mathbf{Y}_{Pk}$ u $i$-tom stupcu i $j$-tom retku.
Pogrešku uprosječujemo po svih $p$ portfelja i svih 30 trenutaka iz prozora budućih povrata.
Kako bi mogli izračunati srednju kvadratnu pogrešku prvi dekoder će izračunati $\widehat{\mathbf{Y}}_{Pk}$
na sljedeći način:
\begin{equation}
  \widehat{\mathbf{Y}}_{Pk} = \mathbf{1}_{30} \cdot \boldsymbol{\alpha}^\intercal + \mathbf{y}_{Mk} \cdot
  \boldsymbol{\beta}^\intercal.
\end{equation}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/mse-decoder.pdf}
\caption{MSE Dekoder}
\label{fig:mse-decoder}
\end{figure}

Druga funckija gubitka je negativna logaritamska izglednost \engl{ negative log likelihood, NLL}. Negativnu
logaritamsku izglednost našeg modela računamo na sljedeći način:
\begin{equation}
  L_{NLL}\left(\widehat{\boldsymbol{\mu}}, \widehat{\boldsymbol{\Sigma}}^{-1}, \mathbf{Y}_{Pk}\right)
  = -\frac{1}{30} \ln \mathcal{L} \left(\widehat{\boldsymbol{\mu}}, \widehat{\boldsymbol{\Sigma}}^{-1};
  \mathbf{Y}_{Pk}\right).
\end{equation}
Pri tome smo se poslužili jednadžbom za računanje logaritamske izglednosti (\ref{eq:log-likelihood}).
Dobivenu logaritamsku izglednost zatim je potrebno uprosječiti na svih 30 realizacija prozora
budućih povrata. To radimo kako bi zagladili moguće ekstremne vrijednosti u prozoru i olakšali
učenje modela.
Drugi dekoder računa procjene $\widehat{\boldsymbol{\mu}}$ i $\widehat{\boldsymbol{\Sigma}}^{-1}$
primjenjujući formule (\ref{eq:ocekivanje-r}) i (\ref{eq:kovarijacijska-r}) na $\boldsymbol{\alpha}$,
$\boldsymbol{\beta}$ i podatke iz skupa $\mathbf{Y}_k$. Kod računanja inverza
$\widehat{\boldsymbol{\Sigma}}^{-1}$ poslužili smo se Woodburyjevom lemom o invertiranju matrica
\cite{statlect-woodbury}.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/nll-decoder.pdf}
\caption{NLL Dekoder}
\label{fig:nll-decoder}
\end{figure}

Treća funckija gubitka je zapravo samo kombinacija srednje kvadratne pogreške i negativne logaritamske
izglednosti te je računamo na idući način:
\begin{equation}
  L_{MIX}\left(\widehat{\mathbf{Y}}_{Pk}, \mathbf{Y}_{Pk}, \widehat{\boldsymbol{\mu}},
  \widehat{\boldsymbol{\Sigma}}^{-1}\right) = L_{NLL}\left(\widehat{\boldsymbol{\mu}},
  \widehat{\boldsymbol{\Sigma}}^{-1}, \mathbf{Y}_{Pk}\right) +
  \zeta\times L_{MSE}\left(\widehat{\mathbf{Y}}_{Pk}, \mathbf{Y}_{Pk}\right),
\end{equation}
gdje koeficijent $\zeta$ određuje doprinos srednje kvadratne pogreške te nam služi kako bi doveli ta dva
gubitka na isti red veličine. Zbog tog je treći dekoder zapravo samo objedinjenje prvog i drugog dekodera
u jedan veći dekoder.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/combined-decoder.pdf}
\caption{Kombinirani Dekoder}
\label{fig:combined-decoder}
\end{figure}


\subsubsection{Računanje ukupnog gubitka modela}
\label{subsubsec:racunanje-ukupnog}

Sva tri dekodera našeg modela na ulazu prima podatke iz skupa $\mathbf{Y}_k$ i vektore procjenjenih
koeficijenata $\boldsymbol{\alpha}$ i $\boldsymbol{\beta}$ koje dobivamo na izlazu enkodera. Osim izlaza
enkodera, skup $\mathbf{Y}_k$ također sadrži vektore parametara $\boldsymbol{\alpha}_k$ i
$\boldsymbol{\beta}_k$ koji se nalaze u matrici parametara $\widehat{\boldsymbol{\xi}}_k^\intercal$.
Ti paramtetri procjenjeni su metodom najmanjih kvadrata ne budućem prozoru povrata. Naš cilj je da model
daje jednako kvalitetne procjene parametara kao i metoda najmanjih kvadrata ne budućem prozoru povrata,
ali koristeći samo povijesni prozor povrata. Stoga ćemo gubitak izračunat na parametrima
$\boldsymbol{\alpha}_k$ i $\boldsymbol{\beta}_k$ smatrati osnovnom razinom gubitka.

Neka je $\mathbf{o}_{xk}$ izlaz dekodera za ulazne vektore $\boldsymbol{\alpha}$ i $\boldsymbol{\beta}$
dobivene primjenom enkodera na tenzor $\mathbf{X}_k$, a $\mathbf{o}_{yk}$ izlaz dekodera za ulazne vektore
$\boldsymbol{\alpha}_k$ i $\boldsymbol{\beta}_k$ dobivene iz skupa $\mathbf{Y}_k$. Ukupan gubitak modela
koji koristimo u postupku optimizacije za primjer $k$ tada računamo na sljedeći način:
\begin{equation}
  \Delta L\left(\mathbf{o}_{xk}, \mathbf{o}_{yk}, \mathbf{Y}_{Pk}\right) =
  L\left(\mathbf{o}_{xk}, \mathbf{Y}_{Pk}\right) -
  L\left(\mathbf{o}_{yk}, \mathbf{Y}_{Pk}\right)
\end{equation}
Gubitak $\Delta L$ tad predstavlja gubitak našeg modela iznad gubitka metode najmanjih kvadrata
primjenjene na prozor budućih povrata. Na taj način ne kažnjavamo model za inherentni šum
i nepredvidivost podataka.
Općenit oblik našeg modela tada možemo prikazati slikom \ref{fig:general-model}.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/general-model.pdf}
\caption{Općeniti oblik modela}
\label{fig:general-model}
\end{figure}


%-------------------------------------------------------------------------------
\chapter{Rezultati}
\label{pog:rezultati}

Nakon što smo učitali i pripremili podatke kako je navedeno u potpoglavljima \ref{sec:podatci} i
\ref{sec:obrada-podataka} generirali smo sintetički skup podataka s $N = 10000$ uzoraka i $p = 100$
dionica kako je opisano u potpoglavlju \ref{sec:sinteticki}.

Zatim smo proveli postupak optimizacije na našem modelu s različitim konfiguracijama. Ispitane
su sve kombinacije hiperparametera prikazanih u tablici \ref{tbl:hiperparametri} osim kombinacija koje
uključuju LSTM jedinicu s veličinom skrivenog stanja 16 i jednim slojem.
\begin{table}[htb]
\caption{Ispitani hiperparametri}
\label{tbl:hiperparametri}
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{ll}
  \toprule
  \textbf{Hiperparametar} & \textbf{Ispitane vrijednosti} \\
  \midrule
  Skup za treniranje & Stvarni, Sintetički \\
  Veličina skrivenog stanja & 16, 32 \\
  Broj slojeva LSTM jedinice & 1, 2 \\
  Stopa učenja & $10^{-3}$, $10^{-4}$, $10^{-5}$, $10^{-6}$ \\
  Funkcija cilja & $\Delta L_{MSE}$, $\Delta L_{NLL}$, $\Delta L_{MIX}$ \\
  \bottomrule
\end{tabular}
\end{table}
Za algoritam gradijetnog spusta u postupku optimizacije odabrali smo Adam jer često pokazuje dobre rezultate
\cite{du03}. Također smo koristili adaptivnu stopu učenja. Svaki put kad se u dvije uzastopne epohe
pogreška na skupu za validaciju ne poboljša, stopa za učenje se prepolavlja. Modeli su trenirani sve
dok se u deset uzastopnih epoha pogreška na skupu za validaciju ne poboljša.

Ispitivanje performansi modela provedeno je računanjem gubitaka $\Delta L_{MSE}$ i $\Delta L_{NLL}$
za sve dobivene modele na skupu za testiranje stvarnog skupa podataka neovisno o tome jeli model
treniran na stvarnom ili sintetičkom skupu podataka. Prilikom ispitivanja korišteni su parametri
modela koji su ostvarili najmanju pogrešku na skupu za validaciju, a ne parametri dobiveni
na samom kraju treniranja.

Osim što smo ispitali gotove sve kombinacije hiperparametara iz tablice \ref{tbl:hiperparametri},
ispitali smo i koncept zagrijavanja modela. Nakon što istreniramo model na sintetičkom skupu podatka,
model možemo nastaviti trenirati na stvarnom skupu podataka. Ideja ovog pristupa je da generiranjem
velikog sintetičkog skupa podataka možemo povećati skup za treniranje i dovesti model do povoljnog
skupa parametara prije nego što krenemo s treniranjem na stvarnom skupu podataka. Nadamo se da će model
na taj način imati veće šanse pronaći bolja rješenja nego model koji kreće od nasumično inicijaliziranog
skupa parametara.

Pokazalo se da na sintetičkom skupu podataka najmanje gubitke $\Delta L_{MSE}$ i $\Delta L_{NLL}$
postiže model s $\Delta L_{MIX}$ funckijom cilja. Stoga smo, prilikom ispitivanja modela s zagrijavanjem,
korisitli parametre dobivene treniranjem modela s $\Delta L_{MIX}$ funckijom cilja na sintetičkom skupu
podataka. Zatim smo nastvaili treniranje modela koristeći pri tome sve kombinacije hiperparametera
prikazanih u tablici \ref{tbl:hiperparametri} osim kombinacija koje uključuju LSTM jedinicu s veličinom
skrivenog stanja 16 i jednim slojem te kombinacija koje uključuju sintetički skup za treniranje.

Potom smo ispitali performanse modela sa zagrijavanjem i metode najmanjih kvadrata (OLS) na isti način kao
za prethodno spomenute modele. Metoda najmanjih kvadrata na povjesnom prozoru podataka će nam služiti za
usporedbu performansi modela i jedne od standardnih metoda estimacije.

Dobiveni rezulrati prikazani su u tablici \ref{tbl:rezultati}. U tablici je također naveden $\Delta L_{MIX}$
gubitak s koeficijentom $\zeta = 10^5$ radi lakše usporedbe ukupnih performansi.

\begin{table}[htb]
\caption{Rezultati ispitivanja na skupu za testiranje}
\label{tbl:rezultati}
\centering
\begin{tabular}{l ccc ccc ccc}
\toprule
& \multicolumn{9}{c}{Test} \\
\cmidrule(lr){2-10}
& \multicolumn{3}{c}{$\Delta L_{MSE}\ (10^{-5})$} 
& \multicolumn{3}{c}{$\Delta L_{NLL}$} 
& \multicolumn{3}{c}{$\Delta L_{MIX}\ (\zeta = 10^5)$} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& $L_{MSE}$ & $L_{NLL}$ & $L_{MIX}$ & $L_{MSE}$ & $L_{NLL}$ & $L_{MIX}$ & $L_{MSE}$ & $L_{NLL}$ & $L_{MIX}$ \\
& \multicolumn{9}{c}{} \\
Sintetički   & 2.230 & 1.966 & \textbf{1.877} & 2.238 & 2.219 & \textbf{2.209} & 4.468 & 4.185 & \textbf{4.086} \\
Stvarni      & 2.127 & \textbf{1.804} & 1.836 & 2.227 & \textbf{2.203} & 2.208 & 4.354 & \textbf{4.007} & 4.044 \\
Zagrijavanje & 1.245 & 2.599 & \textbf{1.052} & 1.443 & \textbf{1.244} & 1.284 & 2.687 & 3.843 & \textbf{2.336} \\
& \multicolumn{9}{c}{} \\
OLS          & \multicolumn{3}{c}{1.182} & \multicolumn{3}{c}{1.536} & \multicolumn{3}{c}{2.718} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/train_val_loss.pdf}
\caption{Gubitak najboljeg modela na skupu za treniranje i validaciju}
\label{fig:best-model-train-val}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/train_val_loss_mse_nll_models.pdf}
\caption{Gubitak najboljeg MSE i NLL modela na skupu za treniranje i validaciju}
\label{fig:mse-nll-model-train-val}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/model_ols_alpha_est.pdf}
\caption{Procjene koeficijenta $\alpha$}
\label{fig:alpha-estimation}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/model_ols_beta_est.pdf}
\caption{Procjene koeficijenta $\beta$}
\label{fig:beta-estimation}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{img/model_ols_reconstructions.png}
\caption{Rekonstrukcije povrata}
\label{fig:rekonstrukcije-povrata}
\end{figure}


%--- ZAKLJUČAK / CONCLUSION ----------------------------------------------------
\chapter{Zaključak}
\label{pog:zakljucak}

TODO Zaključak

Modeli dubokog učenja mogu konkurirati i nekad čak nadmašiti metodu
najmanjih kvadrata što pokazuje da modeli uspješno modeliraju zavisnosti
u povratima. Bitnije otkriće je da modeli zagrijani na sintetičkim
podatcima bolje generaliziraju i pronalaze bolja rješenja.  


%--- LITERATURA / REFERENCES ---------------------------------------------------

% Literatura se automatski generira iz zadane .bib datoteke / References are automatically generated from the supplied .bib file
% Upiši ime BibTeX datoteke bez .bib nastavka / Enter the name of the BibTeX file without .bib extension
\bibliography{literatura}



%--- SAŽETAK / ABSTRACT --------------------------------------------------------

% Sažetak na hrvatskom
\begin{sazetak}
  U ovom radu ispituje se može li model dubokog učenja uspješno procijeniti koeficijente jednofaktorskog
  modela povrata iz prozora povijesnih podataka, i to bolje nego što to čini standardna metoda najmanjih
  kvadrata primijenjena na istom prozoru. Model se temelji na LSTM arhitekturi koja iz vremenskih nizova
  povrata i tržišnog faktora procjenjuje koeficijente $\alpha$ i $\beta$ koji najbolje odgovaraju
  budućim povratima. Kako bi se prevladao problem ograničene količine stvarnih podataka
  za treniranje, generiran je sintetički skup podataka koji replicira statistička svojstva stvarnih
  financijskih povrata. Modeli su trenirani na stvarnim i sintetičkim podatcima, a ispitan je i pristup
  zagrijavanja modela, treniranje na sintetičkim podatcima prije nastavka treniranja na stvarnim.
  Rezultati pokazuju da modeli dubokog učenja mogu konkurirati metodi najmanjih kvadrata i ponekad je
  nadmašiti, što potvrđuje da modeli uspješno modeliraju zavisnosti u povratima. Ključni nalaz je da
  modeli zagrijani na sintetičkim podatcima bolje generaliziraju i pronalaze bolja rješenja od modela
  treniranih isključivo na stvarnim podatcima.

\end{sazetak}

\begin{kljucnerijeci}
  duboko učenje; LSTM; financijski vremenski nizovi; faktorski modeli; sintetički podatci; metoda najmanjih kvadrata
\end{kljucnerijeci}


% Abstract in English
\begin{abstract}
  This thesis investigates whether a deep learning model can successfully estimate the coefficients of
  a single-factor return model from a historical data window, outperforming ordinary least squares (OLS)
  applied to the same window. The model is based on an LSTM architecture that estimates the coefficients
  $\alpha$ and $\beta$, that best fit future returns, from a historic time series of equity returns
  and a market factor. To overcome the limited amount of real training data, a synthetic dataset
  was generated to replicate the statistical properties of real financial returns. Models were
  trained on both real and synthetic data, and a warmup approach was also evaluated, pre-training on
  synthetic data before continuing training on real data. The results show that deep learning models can
  compete with OLS and sometimes surpass it, confirming that the models successfully capture dependencies
  in asset returns. The key finding is that models pre-trained on synthetic data generalize better and find
  superior solutions compared to models trained exclusively on real data.

\end{abstract}

\begin{keywords}
  deep learning; LSTM; financial time series; factor models; synthetic data; ordinary least squares
\end{keywords}


\end{document}
